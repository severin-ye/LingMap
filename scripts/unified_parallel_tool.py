#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œå¤„ç†æµ‹è¯•å’ŒæŠ¥å‘Šå·¥å…·

é›†æˆåŠŸèƒ½ï¼š
1. å¹¶è¡Œé…ç½®æµ‹è¯•
2. æ€§èƒ½åŸºå‡†æµ‹è¯•
3. å¹¶è¡Œé…ç½®æŠ¥å‘Šç”Ÿæˆ
"""

import os
import sys
import time
import json
import argparse
import logging
from enum import Enum
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from common.utils.parallel_config import ParallelConfig
from common.utils.config_writer import ConfigWriter
from common.utils.thread_monitor import ThreadUsageMonitor
from common.utils.json_loader import JsonLoader


class ParallelToolMode(Enum):
    """å¹¶è¡Œå·¥å…·è¿è¡Œæ¨¡å¼"""
    TEST = "test"         # æµ‹è¯•æ¨¡å¼ï¼šéªŒè¯é…ç½®
    BENCHMARK = "bench"   # åŸºå‡†æµ‹è¯•æ¨¡å¼ï¼šæ¯”è¾ƒæ€§èƒ½
    REPORT = "report"     # æŠ¥å‘Šæ¨¡å¼ï¼šç”Ÿæˆé…ç½®æŠ¥å‘Š
    ALL = "all"           # å…¨éƒ¨è¿è¡Œ


def setup_logging(log_filename=None):
    """
    è®¾ç½®æ—¥å¿—è®°å½•
    
    Args:
        log_filename: æ—¥å¿—æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™æŒ‰æ—¥æœŸç”Ÿæˆ
    """
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    if log_filename is None:
        log_filename = f"parallel_tool_{datetime.now().strftime('%Y%m%d')}.log"
    
    log_file = log_dir / log_filename
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file)
        ]
    )
    
    return logging.getLogger("parallel_tool")


def format_duration(seconds):
    """
    æ ¼å¼åŒ–æ—¶é—´ä¸ºå¯è¯»å½¢å¼
    
    Args:
        seconds: ç§’æ•°
        
    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
    """
    if seconds < 60:
        return f"{seconds:.2f}ç§’"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.2f}åˆ†é’Ÿ"
    else:
        hours = seconds / 3600
        return f"{hours:.2f}å°æ—¶"


def run_module_test(module_name, test_func, *args, **kwargs):
    """
    è¿è¡Œæ¨¡å—æµ‹è¯•
    
    Args:
        module_name: æ¨¡å—åç§°
        test_func: æµ‹è¯•å‡½æ•°
        args: ä½ç½®å‚æ•°
        kwargs: å…³é”®å­—å‚æ•°
        
    Returns:
        æµ‹è¯•ç»“æœå’Œæ‰§è¡Œæ—¶é—´
    """
    print(f"\næµ‹è¯•æ¨¡å—: {module_name}")
    start_time = time.time()
    try:
        result = test_func(*args, **kwargs)
        end_time = time.time()
        duration = end_time - start_time
        print(f"âœ… {module_name} æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {format_duration(duration)}")
        return result, duration
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ {module_name} æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, duration


#--------------------------------------------------------------------
# é…ç½®æµ‹è¯•ç›¸å…³åŠŸèƒ½
#--------------------------------------------------------------------
def test_parallel_config_consistency(logger=None):
    """
    æµ‹è¯•å¹¶è¡Œé…ç½®ä¸€è‡´æ€§
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
    """
    if logger is None:
        logger = logging.getLogger()

    # åˆå§‹åŒ–ParallelConfig
    ParallelConfig.initialize()
    
    # è®°å½•é…ç½®ä¿¡æ¯
    logger.info("====== å¹¶è¡Œé…ç½®æµ‹è¯• ======")
    logger.info(f"å¹¶è¡Œå¤„ç†å¯ç”¨çŠ¶æ€: {ParallelConfig.is_enabled()}")
    logger.info(f"å…¨å±€æœ€å¤§çº¿ç¨‹æ•°: {ParallelConfig._config['max_workers']}")
    
    # è®°å½•å„æ¨¡å—é…ç½®
    logger.info("æ¨¡å—ç‰¹å®šé…ç½®:")
    for module, workers in ParallelConfig._config["default_workers"].items():
        logger.info(f"  - {module}: {workers}")
    
    # æµ‹è¯•å„æ¨¡å—å®ä¾‹
    logger.info("\næµ‹è¯•å„æ¨¡å—å®ä¾‹åŒ–:")
    
    try:
        # äº‹ä»¶æŠ½å–
        logger.info("åˆ›å»ºäº‹ä»¶æŠ½å–å™¨...")
        from event_extraction.di.provider import provide_extractor
        extractor = provide_extractor()
        
        # å¹»è§‰ä¿®å¤
        logger.info("åˆ›å»ºå¹»è§‰ä¿®å¤å™¨...")
        from hallucination_refine.di.provider import provide_refiner
        refiner = provide_refiner()
        
        # å› æœé“¾æ¥
        logger.info("åˆ›å»ºå› æœé“¾æ¥å™¨...")
        from causal_linking.di.provider import provide_linker
        linker = provide_linker()
        
        # å›¾å½¢æ„å»º
        logger.info("åˆ›å»ºå›¾å½¢æ¸²æŸ“å™¨...")
        from graph_builder.service.mermaid_renderer import MermaidRenderer
        renderer = MermaidRenderer()
        
        logger.info("æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def test_config_updates(logger=None):
    """
    æµ‹è¯•é…ç½®æ›´æ–°
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        æ˜¯å¦æµ‹è¯•æˆåŠŸ
    """
    if logger is None:
        logger = logging.getLogger()
    
    try:
        logger.info("\n====== é…ç½®æ›´æ–°æµ‹è¯• ======")
        
        # è®°å½•åŸå§‹é…ç½®
        original_max = ParallelConfig._config["max_workers"]
        original_graph = ParallelConfig._config["default_workers"]["graph_builder"]
        logger.info(f"åŸå§‹çº¿ç¨‹é…ç½®: å…¨å±€={original_max}, å›¾å½¢æ„å»º={original_graph}")
        
        # æ›´æ–°é…ç½®
        test_updates = {
            "max_workers": original_max + 2,
            "default_workers": {
                "graph_builder": original_graph + 1
            }
        }
        logger.info(f"æ›´æ–°é…ç½®: {test_updates}")
        
        # åº”ç”¨æ›´æ–°
        ConfigWriter.update_parallel_config(test_updates)
        
        # éªŒè¯æ›´æ–°åçš„é…ç½®
        logger.info(f"æ›´æ–°åé…ç½®: å…¨å±€={ParallelConfig._config['max_workers']}, " +
                   f"å›¾å½¢æ„å»º={ParallelConfig._config['default_workers']['graph_builder']}")
        
        # æ¢å¤åŸå§‹é…ç½®
        restore_updates = {
            "max_workers": original_max,
            "default_workers": {
                "graph_builder": original_graph
            }
        }
        logger.info(f"æ¢å¤åŸå§‹é…ç½®: {restore_updates}")
        ConfigWriter.update_parallel_config(restore_updates)
        
        # ç¡®è®¤æ¢å¤æˆåŠŸ
        logger.info(f"æ¢å¤åé…ç½®: å…¨å±€={ParallelConfig._config['max_workers']}, " +
                   f"å›¾å½¢æ„å»º={ParallelConfig._config['default_workers']['graph_builder']}")
        return True
    except Exception as e:
        logger.error(f"é…ç½®æ›´æ–°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


#--------------------------------------------------------------------
# æŠ¥å‘Šç”Ÿæˆç›¸å…³åŠŸèƒ½
#--------------------------------------------------------------------
def generate_parallel_report():
    """
    ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š
    
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    # åˆå§‹åŒ–å¹¶è¡Œé…ç½®
    ParallelConfig.initialize()
    
    # åˆå§‹åŒ–çº¿ç¨‹ç›‘æ§
    thread_monitor = ThreadUsageMonitor.get_instance()
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    report_dir = Path("reports")
    report_dir.mkdir(exist_ok=True)
    
    report_file = report_dir / f"parallel_config_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    # æ”¶é›†é…ç½®ä¿¡æ¯
    config_info = {
        "enabled": ParallelConfig.is_enabled(),
        "max_workers": ParallelConfig._config["max_workers"],
        "adaptive": ParallelConfig._config["adaptive"],
        "default_workers": ParallelConfig._config["default_workers"]
    }
    
    # ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š
    logging.info("ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š...")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç³»ç»Ÿå¹¶è¡Œå¤„ç†é…ç½®æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # åŸºæœ¬é…ç½®
        f.write("## åŸºæœ¬é…ç½®\n\n")
        f.write(f"- å¹¶è¡Œå¤„ç†çŠ¶æ€: {'å¯ç”¨' if config_info['enabled'] else 'ç¦ç”¨'}\n")
        f.write(f"- å…¨å±€æœ€å¤§çº¿ç¨‹æ•°: {config_info['max_workers']}\n")
        
        # è‡ªé€‚åº”é…ç½®
        f.write("\n## è‡ªé€‚åº”çº¿ç¨‹é…ç½®\n\n")
        adaptive = config_info['adaptive']
        f.write(f"- è‡ªé€‚åº”æ¨¡å¼: {'å¯ç”¨' if adaptive['enabled'] else 'ç¦ç”¨'}\n")
        if adaptive['enabled']:
            f.write(f"- IOå¯†é›†å‹ä»»åŠ¡ç³»æ•°: {adaptive['io_bound_factor']}\n")
            f.write(f"- CPUå¯†é›†å‹ä»»åŠ¡ç³»æ•°: {adaptive['cpu_bound_factor']}\n")
            
            io_threads = int(config_info['max_workers'] * adaptive['io_bound_factor'])
            cpu_threads = int(config_info['max_workers'] * adaptive['cpu_bound_factor'])
            
            f.write(f"- IOå¯†é›†å‹ä»»åŠ¡çº¿ç¨‹æ•°: {io_threads}\n")
            f.write(f"- CPUå¯†é›†å‹ä»»åŠ¡çº¿ç¨‹æ•°: {cpu_threads}\n")
        
        # æ¨¡å—ç‰¹å®šé…ç½®
        f.write("\n## æ¨¡å—ç‰¹å®šé…ç½®\n\n")
        f.write("| æ¨¡å— | é…ç½®çº¿ç¨‹æ•° |\n")
        f.write("|------|----------|\n")
        
        for module, workers in config_info['default_workers'].items():
            f.write(f"| {module} | {workers} |\n")
        
        # åˆå§‹åŒ–å„æ¨¡å—å¹¶è®°å½•çº¿ç¨‹ä½¿ç”¨
        f.write("\n## å®é™…çº¿ç¨‹ä½¿ç”¨æƒ…å†µ\n\n")
        f.write("ç°åœ¨å¼€å§‹æµ‹è¯•å„æ¨¡å—å®é™…ä½¿ç”¨çš„çº¿ç¨‹æ•°...\n\n")
        
        # äº‹ä»¶æŠ½å–
        logging.info("æµ‹è¯•äº‹ä»¶æŠ½å–æ¨¡å—...")
        f.write("### äº‹ä»¶æŠ½å–æ¨¡å—\n\n")
        try:
            from event_extraction.di.provider import provide_extractor
            extractor = provide_extractor()
            f.write("âœ… äº‹ä»¶æŠ½å–æ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ äº‹ä»¶æŠ½å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # å¹»è§‰ä¿®å¤
        logging.info("æµ‹è¯•å¹»è§‰ä¿®å¤æ¨¡å—...")
        f.write("\n### å¹»è§‰ä¿®å¤æ¨¡å—\n\n")
        try:
            from hallucination_refine.di.provider import provide_refiner
            refiner = provide_refiner()
            f.write("âœ… å¹»è§‰ä¿®å¤æ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ å¹»è§‰ä¿®å¤æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # å› æœé“¾æ¥
        logging.info("æµ‹è¯•å› æœé“¾æ¥æ¨¡å—...")
        f.write("\n### å› æœé“¾æ¥æ¨¡å—\n\n")
        try:
            from causal_linking.di.provider import provide_linker
            linker = provide_linker()
            f.write("âœ… å› æœé“¾æ¥æ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ å› æœé“¾æ¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # å›¾å½¢æ„å»º
        logging.info("æµ‹è¯•å›¾å½¢æ„å»ºæ¨¡å—...")
        f.write("\n### å›¾å½¢æ„å»ºæ¨¡å—\n\n")
        try:
            from graph_builder.service.mermaid_renderer import MermaidRenderer
            renderer = MermaidRenderer()
            f.write("âœ… å›¾å½¢æ„å»ºæ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ å›¾å½¢æ„å»ºæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # è·å–å¹¶è®°å½•çº¿ç¨‹ç›‘æ§ä¿¡æ¯
        usage_info = thread_monitor.thread_usage
        
        f.write("\n## çº¿ç¨‹ä½¿ç”¨æ‘˜è¦\n\n")
        f.write("| æ¨¡å— | é…ç½®çº¿ç¨‹æ•° | å®é™…ä½¿ç”¨çº¿ç¨‹æ•° | ä»»åŠ¡ç±»å‹ |\n")
        f.write("|------|------------|--------------|--------|\n")
        
        for module, workers in config_info['default_workers'].items():
            actual_workers = usage_info.get(module, {}).get("thread_count", "æœªçŸ¥")
            task_type = usage_info.get(module, {}).get("task_type", "æœªçŸ¥")
            f.write(f"| {module} | {workers} | {actual_workers} | {task_type} |\n")
        
        # ç»“è®ºå’Œå»ºè®®
        f.write("\n## ç»“è®ºå’Œå»ºè®®\n\n")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å—æœªä½¿ç”¨é›†ä¸­é…ç½®
        unconfigured_modules = set(usage_info.keys()) - set(config_info['default_workers'].keys())
        if unconfigured_modules:
            f.write("âš ï¸ ä»¥ä¸‹æ¨¡å—æœªä½¿ç”¨ä¸­å¤®é…ç½®:\n\n")
            for module in unconfigured_modules:
                f.write(f"- {module}\n")
            f.write("\nå»ºè®®å°†è¿™äº›æ¨¡å—æ·»åŠ åˆ°ä¸­å¤®é…ç½®ä¸­ã€‚\n\n")
        
        # æ£€æŸ¥é…ç½®ä¸ä½¿ç”¨æ˜¯å¦ä¸€è‡´
        inconsistent_modules = []
        for module, info in usage_info.items():
            if module in config_info['default_workers']:
                expected = config_info['default_workers'][module]
                actual = info.get("thread_count", 0)
                if expected != actual and ParallelConfig.is_enabled():
                    inconsistent_modules.append((module, expected, actual))
                    
        if inconsistent_modules:
            f.write("âš ï¸ ä»¥ä¸‹æ¨¡å—çš„çº¿ç¨‹ä½¿ç”¨ä¸é…ç½®ä¸ä¸€è‡´:\n\n")
            for module, expected, actual in inconsistent_modules:
                f.write(f"- {module}: æœŸæœ› {expected}ï¼Œå®é™… {actual}\n")
            f.write("\nå»ºè®®æ£€æŸ¥è¿™äº›æ¨¡å—çš„å¹¶è¡Œå®ç°æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†ParallelConfigã€‚\n\n")
        
        # é€‚åº”æ€§å»ºè®®
        f.write("### ä¼˜åŒ–å»ºè®®\n\n")
        f.write("æ ¹æ®æ¨¡å—ä»»åŠ¡ç‰¹æ€§çš„ä¸åŒï¼Œå»ºè®®ä»¥ä¸‹çº¿ç¨‹é…ç½®ï¼š\n\n")
        f.write("- IOå¯†é›†å‹ä»»åŠ¡ (å¦‚APIè°ƒç”¨): æ ¸å¿ƒæ•° x 1.5\n")
        f.write("- CPUå¯†é›†å‹ä»»åŠ¡ (å¦‚å›¾å½¢æ¸²æŸ“): æ ¸å¿ƒæ•° x 0.8\n")
        f.write("- æ··åˆå‹ä»»åŠ¡: ä¸æ ¸å¿ƒæ•°ç›¸å½“\n\n")
        
        f.write("å½“å‰ç³»ç»Ÿä¸­çš„åˆ†ç±»ï¼š\n\n")
        f.write("- IOå¯†é›†å‹ï¼ševent_extraction, hallucination_refine\n")
        f.write("- CPUå¯†é›†å‹ï¼šgraph_builder\n")
        f.write("- æ··åˆå‹ï¼šcausal_linking\n")
    
    logging.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"ğŸ“Š å¹¶è¡Œé…ç½®æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    return report_file


#--------------------------------------------------------------------
# åŸºå‡†æµ‹è¯•ç›¸å…³åŠŸèƒ½
#--------------------------------------------------------------------
def test_event_extraction(chapter_file):
    """
    æµ‹è¯•äº‹ä»¶æå–æ¨¡å—
    
    Args:
        chapter_file: ç« èŠ‚æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„äº‹ä»¶
    """
    from text_ingestion.chapter_loader import ChapterLoader
    from event_extraction.di.provider import provide_extractor
    
    # åŠ è½½ç« èŠ‚
    loader = ChapterLoader(segment_size=800)
    chapter = loader.load_from_json(chapter_file)
    
    if not chapter:
        raise ValueError("åŠ è½½ç« èŠ‚å¤±è´¥")
    
    # æå–äº‹ä»¶
    extractor = provide_extractor()
    print(f"ä»ç« èŠ‚ {chapter.chapter_id} æå–äº‹ä»¶...")
    events = extractor.extract(chapter)
    print(f"æˆåŠŸæå– {len(events)} ä¸ªäº‹ä»¶")
    
    return events


def test_hallucination_refine(events, context):
    """
    æµ‹è¯•å¹»è§‰ä¿®å¤æ¨¡å—
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
    Returns:
        ç²¾ä¿®åçš„äº‹ä»¶
    """
    from hallucination_refine.di.provider import provide_refiner
    
    refiner = provide_refiner()
    print(f"å¯¹ {len(events)} ä¸ªäº‹ä»¶è¿›è¡Œå¹»è§‰æ£€æµ‹å’Œä¿®å¤...")
    refined_events = refiner.refine(events, context=context)
    print(f"ç²¾ä¿®å®Œæˆï¼Œå…± {len(refined_events)} ä¸ªäº‹ä»¶")
    
    return refined_events


def test_causal_linking(events):
    """
    æµ‹è¯•å› æœåˆ†ææ¨¡å—
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
        
    Returns:
        äº‹ä»¶å’Œè¾¹çš„å…ƒç»„
    """
    from causal_linking.di.provider import provide_linker
    
    linker = provide_linker(use_optimized=True)
    print(f"åˆ†æ {len(events)} ä¸ªäº‹ä»¶ä¹‹é—´çš„å› æœå…³ç³»...")
    edges = linker.link_events(events)
    print(f"å‘ç° {len(edges)} ä¸ªå› æœå…³ç³»")
    
    # æ„å»ºDAG
    print("æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰...")
    events, dag_edges = linker.build_dag(events, edges)
    print(f"DAGæ„å»ºå®Œæˆï¼Œä¿ç•™ {len(dag_edges)} æ¡è¾¹")
    
    return events, dag_edges


def test_graph_rendering(events, edges):
    """
    æµ‹è¯•å›¾å½¢æ¸²æŸ“æ¨¡å—
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
        edges: è¾¹åˆ—è¡¨
        
    Returns:
        æ¸²æŸ“çš„Mermaidæ–‡æœ¬
    """
    from graph_builder.service.mermaid_renderer import MermaidRenderer
    
    renderer = MermaidRenderer()
    options = {
        "show_legend": True,
        "show_edge_labels": True,
        "custom_edge_style": True
    }
    
    print(f"æ¸²æŸ“ {len(events)} ä¸ªäº‹ä»¶èŠ‚ç‚¹å’Œ {len(edges)} æ¡è¾¹...")
    mermaid_text = renderer.render(events, edges, options)
    
    return mermaid_text


def run_benchmark(args):
    """
    è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    # è®¾ç½®è¾“å…¥æ–‡ä»¶è·¯å¾„
    if args.input:
        chapter_file = args.input
    else:
        # æŸ¥æ‰¾æµ‹è¯•æ•°æ®
        temp_dir = os.path.join(project_root, "temp")
        output_dirs = [d for d in os.listdir(os.path.join(project_root, "output")) 
                       if os.path.isdir(os.path.join(project_root, "output", d))]
        
        if output_dirs:
            # ä½¿ç”¨æœ€æ–°çš„è¾“å‡ºç›®å½•
            latest_dir = sorted(output_dirs)[-1]
            temp_dir = os.path.join(project_root, "output", latest_dir, "temp")
            
        # æŸ¥æ‰¾ç« èŠ‚JSONæ–‡ä»¶
        json_files = [f for f in os.listdir(temp_dir) 
                     if os.path.isfile(os.path.join(temp_dir, f))
                     and f.endswith('.json') and 'chapter' in f.lower()]
        
        if not json_files:
            # å°è¯•æŸ¥æ‰¾ä»»æ„JSONæ–‡ä»¶
            json_files = [f for f in os.listdir(temp_dir) 
                         if os.path.isfile(os.path.join(temp_dir, f))
                         and f.endswith('.json')]
            
        if not json_files:
            raise FileNotFoundError("æ‰¾ä¸åˆ°æµ‹è¯•ç”¨çš„ç« èŠ‚JSONæ–‡ä»¶")
            
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„JSONæ–‡ä»¶
        chapter_file = os.path.join(temp_dir, json_files[0])
        
    print(f"ä½¿ç”¨æµ‹è¯•æ•°æ®: {chapter_file}")
    
    # è¿è¡Œå¹¶è¡Œæ¨¡å¼æµ‹è¯•
    print("===== å¹¶è¡Œæ¨¡å¼æµ‹è¯• =====")
    
    # ç¡®ä¿å¹¶è¡Œæ¨¡å¼å·²å¯ç”¨
    ParallelConfig.initialize({"enabled": True})
    print(f"å¹¶è¡Œæ¨¡å¼: å¯ç”¨ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {ParallelConfig._config['max_workers']}")
    
    # ä¿å­˜å„æµ‹è¯•é˜¶æ®µçš„æ‰§è¡Œæ—¶é—´
    parallel_results = {}
    
    # äº‹ä»¶æŠ½å–é˜¶æ®µ
    events, duration = run_module_test("äº‹ä»¶æŠ½å–", test_event_extraction, chapter_file)
    parallel_results["äº‹ä»¶æŠ½å–"] = duration
    
    if not events:
        print("äº‹ä»¶æŠ½å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
    
    # æå–ç« èŠ‚ä¸Šä¸‹æ–‡ç”¨äºå¹»è§‰æ£€æµ‹
    context = "æµ‹è¯•ä¸Šä¸‹æ–‡"
    try:
        with open(chapter_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict) and "content" in data:
                context = data["content"][:500] + "..." if len(data["content"]) > 500 else data["content"]
    except:
        print("æå–ä¸Šä¸‹æ–‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡")
    
    # å¹»è§‰ä¿®å¤é˜¶æ®µ
    refined_events, duration = run_module_test("å¹»è§‰ä¿®å¤", test_hallucination_refine, events, context)
    parallel_results["å¹»è§‰ä¿®å¤"] = duration
    
    if not refined_events:
        refined_events = events
    
    # å› æœé“¾æ¥é˜¶æ®µ
    linking_result, duration = run_module_test("å› æœé“¾æ¥", test_causal_linking, refined_events)
    parallel_results["å› æœé“¾æ¥"] = duration
    
    if not linking_result:
        print("å› æœé“¾æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
        
    events, edges = linking_result
    
    # å›¾è°±æ¸²æŸ“é˜¶æ®µ
    mermaid_text, duration = run_module_test("å›¾è°±æ¸²æŸ“", test_graph_rendering, events, edges)
    parallel_results["å›¾è°±æ¸²æŸ“"] = duration
    
    # å¦‚æœä¸è·³è¿‡é¡ºåºæµ‹è¯•ï¼Œåˆ™è¿›è¡Œé¡ºåºæ¨¡å¼æµ‹è¯•
    sequential_results = {}
    if not args.skip_sequential:
        print("\n===== é¡ºåºæ¨¡å¼æµ‹è¯• =====")
        
        # åˆ‡æ¢åˆ°é¡ºåºæ¨¡å¼
        ParallelConfig.initialize({"enabled": False})
        print("é¡ºåºæ¨¡å¼: å¯ç”¨")
        
        # åŒæ ·çš„æµ‹è¯•æµç¨‹
        events, duration = run_module_test("äº‹ä»¶æŠ½å–(é¡ºåº)", test_event_extraction, chapter_file)
        sequential_results["äº‹ä»¶æŠ½å–"] = duration
        
        if not events:
            print("äº‹ä»¶æŠ½å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # å¹»è§‰ä¿®å¤é˜¶æ®µ
        refined_events, duration = run_module_test("å¹»è§‰ä¿®å¤(é¡ºåº)", test_hallucination_refine, events, context)
        sequential_results["å¹»è§‰ä¿®å¤"] = duration
        
        if not refined_events:
            refined_events = events
        
        # å› æœé“¾æ¥é˜¶æ®µ
        linking_result, duration = run_module_test("å› æœé“¾æ¥(é¡ºåº)", test_causal_linking, refined_events)
        sequential_results["å› æœé“¾æ¥"] = duration
        
        if not linking_result:
            print("å› æœé“¾æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
            
        events, edges = linking_result
        
        # å›¾è°±æ¸²æŸ“é˜¶æ®µ
        mermaid_text, duration = run_module_test("å›¾è°±æ¸²æŸ“(é¡ºåº)", test_graph_rendering, events, edges)
        sequential_results["å›¾è°±æ¸²æŸ“"] = duration
    
    # é‡æ–°å¯ç”¨å¹¶è¡Œæ¨¡å¼
    ParallelConfig.initialize({"enabled": True})
    
    # ç”ŸæˆæŠ¥å‘Š
    report_content = generate_benchmark_report(parallel_results, sequential_results)
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    report_dir = Path("reports")
    report_dir.mkdir(exist_ok=True)
    
    # å†™å…¥æŠ¥å‘Š
    report_file = None
    if args.output:
        report_file = args.output
    else:
        report_file = report_dir / f"parallel_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
        
    print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
    return report_file


def generate_benchmark_report(parallel_results, sequential_results):
    """
    ç”Ÿæˆæ€§èƒ½æ¯”è¾ƒæŠ¥å‘Š
    
    Args:
        parallel_results: å¹¶è¡Œæ¨¡å¼æµ‹è¯•ç»“æœ
        sequential_results: é¡ºåºæ¨¡å¼æµ‹è¯•ç»“æœ
        
    Returns:
        æŠ¥å‘Šæ–‡æœ¬å†…å®¹
    """
    report = []
    report.append("# å¹¶è¡Œå¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # æ€§èƒ½æ‘˜è¦
    report.append("## æ€§èƒ½æ‘˜è¦")
    total_parallel = sum(parallel_results.values())
    total_sequential = sum(sequential_results.values()) if sequential_results else 0
    
    report.append(f"- å¹¶è¡Œå¤„ç†æ€»è€—æ—¶: {format_duration(total_parallel)}")
    if sequential_results:
        speedup = (total_sequential / total_parallel) if total_parallel > 0 else 0
        report.append(f"- é¡ºåºå¤„ç†æ€»è€—æ—¶: {format_duration(total_sequential)}")
        report.append(f"- åŠ é€Ÿæ¯”: {speedup:.2f}x")
        report.append(f"- æ€§èƒ½æå‡: {(speedup - 1) * 100:.2f}%")
    else:
        report.append("- é¡ºåºå¤„ç†æµ‹è¯•å·²è·³è¿‡")
    report.append("")
    
    # æ¨¡å—æ€§èƒ½æ¯”è¾ƒ
    if sequential_results:
        report.append("## å„æ¨¡å—æ€§èƒ½æ¯”è¾ƒ")
        report.append("| æ¨¡å— | å¹¶è¡Œå¤„ç†è€—æ—¶ | é¡ºåºå¤„ç†è€—æ—¶ | åŠ é€Ÿæ¯” | æå‡ç™¾åˆ†æ¯” |")
        report.append("| --- | ------- | ------- | ----- | ------- |")
        
        for module in parallel_results.keys():
            par_time = parallel_results[module]
            seq_time = sequential_results.get(module, 0)
            if seq_time > 0 and par_time > 0:
                mod_speedup = seq_time / par_time
                improvement = (mod_speedup - 1) * 100
                report.append(f"| {module} | {format_duration(par_time)} | {format_duration(seq_time)} | {mod_speedup:.2f}x | {improvement:.2f}% |")
    else:
        report.append("## å¹¶è¡Œæ¨¡å¼æ‰§è¡Œæ—¶é—´")
        report.append("| æ¨¡å— | å¹¶è¡Œå¤„ç†è€—æ—¶ |")
        report.append("| --- | ------- |")
        for module, time in parallel_results.items():
            report.append(f"| {module} | {format_duration(time)} |")
    
    report.append("")
    report.append("## æµ‹è¯•é…ç½®")
    report.append(f"- CPUæ ¸å¿ƒæ•°: {os.cpu_count()}")
    report.append(f"- å¹¶è¡Œæ¨¡å¼å·¥ä½œçº¿ç¨‹æ•°:")
    report.append(f"  - äº‹ä»¶æå–: {ParallelConfig.get_max_workers('io_bound')}")
    report.append(f"  - å¹»è§‰ä¿®å¤: {ParallelConfig.get_max_workers('io_bound')}")
    report.append(f"  - å› æœåˆ†æ: {ParallelConfig.get_max_workers()}")
    report.append(f"  - å›¾è°±æ¸²æŸ“: {ParallelConfig.get_max_workers('cpu_bound')}")
    
    return '\n'.join(report)


def main():
    """ç¨‹åºä¸»å…¥å£"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="å¹¶è¡Œå¤„ç†å·¥å…·")
    parser.add_argument("mode", choices=["test", "bench", "report", "all"], default="all", 
                        nargs="?", help="è¿è¡Œæ¨¡å¼ï¼štest-é…ç½®æµ‹è¯•ï¼Œbench-æ€§èƒ½æµ‹è¯•ï¼Œreport-ç”ŸæˆæŠ¥å‘Šï¼Œall-å…¨éƒ¨è¿è¡Œ")
    parser.add_argument("--input", "-i", help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (ç”¨äºæ€§èƒ½æµ‹è¯•)")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--skip-sequential", action="store_true", help="è·³è¿‡é¡ºåºå¤„ç†æµ‹è¯•")
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    logger.info(f"è¿è¡Œå¹¶è¡Œå¤„ç†å·¥å…·ï¼Œæ¨¡å¼: {args.mode}")
    
    # è®°å½•å¯åŠ¨æ—¶é—´
    start_time = time.time()
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œä¸åŒåŠŸèƒ½
    try:
        mode = ParallelToolMode(args.mode)
        
        if mode in [ParallelToolMode.TEST, ParallelToolMode.ALL]:
            logger.info("==== è¿è¡Œå¹¶è¡Œé…ç½®æµ‹è¯• ====")
            test_result = test_parallel_config_consistency(logger)
            if test_result:
                config_result = test_config_updates(logger)
                if config_result:
                    print("âœ… é…ç½®æµ‹è¯•æˆåŠŸ")
                else:
                    print("âŒ é…ç½®æ›´æ–°æµ‹è¯•å¤±è´¥")
            else:
                print("âŒ é…ç½®ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥")
                
        if mode in [ParallelToolMode.REPORT, ParallelToolMode.ALL]:
            logger.info("==== ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š ====")
            report_file = generate_parallel_report()
            print(f"âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {report_file}")
            
        if mode in [ParallelToolMode.BENCHMARK, ParallelToolMode.ALL]:
            logger.info("==== è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯• ====")
            benchmark_file = run_benchmark(args)
            print(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ: {benchmark_file}")
            
    except Exception as e:
        logger.error(f"è¿è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        
    # è®°å½•æ€»æ‰§è¡Œæ—¶é—´
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {format_duration(duration)}")
    print(f"æ€»æ‰§è¡Œæ—¶é—´: {format_duration(duration)}")


if __name__ == "__main__":
    main()
