#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œå¤„ç†æµ‹è¯•å’ŒæŠ¥å‘Šå·¥å…·

é›†æˆåŠŸèƒ½ï¼š
1. å¹¶è¡Œé…ç½®æµ‹è¯•
2. æ€§èƒ½åŸºå‡†æµ‹è¯•
3. å¹¶è¡Œé…ç½®æŠ¥å‘Šç”Ÿæˆ
"""

import os
import sys
import time
import json
import argparse
import logging
from enum import Enum
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

# TODO: Translate - Add project root directory toç³»ç»Ÿè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from common.utils.parallel_config import ParallelConfig
from common.utils.config_writer import ConfigWriter
from common.utils.thread_monitor import ThreadUsageMonitor
from common.utils.json_loader import JsonLoader


class ParallelToolMode(Enum):
    """å¹¶è¡Œå·¥å…·è¿è¡Œæ¨¡å¼"""
    TEST = "test"         # TODO: Translate - Testæ¨¡å¼ï¼šVerifyConfigure
    BENCHMARK = "bench"   # TODO: Translate - åŸºå‡†Testæ¨¡å¼ï¼šæ¯”è¾ƒæ€§èƒ½
    REPORT = "report"     # TODO: Translate - æŠ¥å‘Šæ¨¡å¼ï¼šGenerateConfigureæŠ¥å‘Š
    ALL = "all"           # TODO: Translate - å…¨éƒ¨Run


def setup_logging(log_filename=None):
    """
    è®¾ç½®æ—¥å¿—è®°å½•
    
    Args:
        log_filename: æ—¥å¿—æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™æŒ‰æ—¥æœŸç”Ÿæˆ
    """
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    if log_filename is None:
        log_filename = f"parallel_tool_{datetime.now().strftime('%Y%m%d')}.log"
    
    log_file = log_dir / log_filename
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file)
        ]
    )
    
    return logging.getLogger("parallel_tool")


def format_duration(seconds):
    """
    æ ¼å¼åŒ–æ—¶é—´ä¸ºå¯è¯»å½¢å¼
    
    Args:
        seconds: ç§’æ•°
        
    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
    """
    if seconds < 60:
        return f"{seconds:.2f}ç§’"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.2f}åˆ†é’Ÿ"
    else:
        hours = seconds / 3600
        return f"{hours:.2f}å°æ—¶"


def run_module_test(module_name, test_func, *args, **kwargs):
    """
    è¿è¡Œæ¨¡å—æµ‹è¯•
    
    Args:
        module_name: æ¨¡å—åç§°
        test_func: æµ‹è¯•å‡½æ•°
        args: ä½ç½®å‚æ•°
        kwargs: å…³é”®å­—å‚æ•°
        
    Returns:
        æµ‹è¯•ç»“æœå’Œæ‰§è¡Œæ—¶é—´
    """
    print(f"\næµ‹è¯•æ¨¡å—: {module_name}")
    start_time = time.time()
    try:
        result = test_func(*args, **kwargs)
        end_time = time.time()
        duration = end_time - start_time
        print(f"âœ… {module_name} æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {format_duration(duration)}")
        return result, duration
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ {module_name} æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, duration


#--------------------------------------------------------------------
# TODO: Translate - ConfigureTestç›¸å…³åŠŸèƒ½
#--------------------------------------------------------------------
def test_parallel_config_consistency(logger=None):
    """
    æµ‹è¯•å¹¶è¡Œé…ç½®ä¸€è‡´æ€§
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
    """
    if logger is None:
        logger = logging.getLogger()

    # InitializeParallelConfig
    ParallelConfig.initialize()
    
    # TODO: Translate - è®°å½•Configureä¿¡æ¯
    logger.info("====== å¹¶è¡Œé…ç½®æµ‹è¯• ======")
    logger.info(f"å¹¶è¡Œå¤„ç†å¯ç”¨çŠ¶æ€: {ParallelConfig.is_enabled()}")
    logger.info(f"å…¨å±€æœ€å¤§çº¿ç¨‹æ•°: {ParallelConfig._config['max_workers']}")
    
    # TODO: Translate - è®°å½•å„æ¨¡å—Configure
    logger.info("æ¨¡å—ç‰¹å®šé…ç½®:")
    for module, workers in ParallelConfig._config["default_workers"].items():
        logger.info(f"  - {module}: {workers}")
    
    # TODO: Translate - Testå„æ¨¡å—å®ä¾‹
    logger.info("\næµ‹è¯•å„æ¨¡å—å®ä¾‹åŒ–:")
    
    try:
        # eventExtract
        logger.info("åˆ›å»ºäº‹ä»¶æŠ½å–å™¨...")
        from event_extraction.di.provider import provide_extractor
        extractor = provide_extractor()
        
        # hallucinationrefine
        logger.info("åˆ›å»ºå¹»è§‰ä¿®å¤å™¨...")
        from hallucination_refine.di.provider import provide_refiner
        refiner = provide_refiner()
        
        # causallinking
        logger.info("åˆ›å»ºå› æœé“¾æ¥å™¨...")
        from causal_linking.di.provider import provide_linker
        linker = provide_linker()
        
        # TODO: Translate - å›¾å½¢Build
        logger.info("åˆ›å»ºå›¾å½¢æ¸²æŸ“å™¨...")
        from graph_builder.service.mermaid_renderer import MermaidRenderer
        renderer = MermaidRenderer()
        
        logger.info("æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def test_config_updates(logger=None):
    """
    æµ‹è¯•é…ç½®æ›´æ–°
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        æ˜¯å¦æµ‹è¯•æˆåŠŸ
    """
    if logger is None:
        logger = logging.getLogger()
    
    try:
        logger.info("\n====== é…ç½®æ›´æ–°æµ‹è¯• ======")
        
        # TODO: Translate - è®°å½•åŸå§‹Configure
        original_max = ParallelConfig._config["max_workers"]
        original_graph = ParallelConfig._config["default_workers"]["graph_builder"]
        logger.info(f"åŸå§‹çº¿ç¨‹é…ç½®: å…¨å±€={original_max}, å›¾å½¢æ„å»º={original_graph}")
        
        # TODO: Translate - æ›´æ–°Configure
        test_updates = {
            "max_workers": original_max + 2,
            "default_workers": {
                "graph_builder": original_graph + 1
            }
        }
        logger.info(f"æ›´æ–°é…ç½®: {test_updates}")
        
        # TODO: Translate - åº”ç”¨æ›´æ–°
        ConfigWriter.update_parallel_config(test_updates)
        
        # TODO: Translate - Verifyæ›´æ–°åçš„Configure
        logger.info(f"æ›´æ–°åé…ç½®: å…¨å±€={ParallelConfig._config['max_workers']}, " +
                   f"å›¾å½¢æ„å»º={ParallelConfig._config['default_workers']['graph_builder']}")
        
        # TODO: Translate - æ¢å¤åŸå§‹Configure
        restore_updates = {
            "max_workers": original_max,
            "default_workers": {
                "graph_builder": original_graph
            }
        }
        logger.info(f"æ¢å¤åŸå§‹é…ç½®: {restore_updates}")
        ConfigWriter.update_parallel_config(restore_updates)
        
        # TODO: Translate - ç¡®è®¤æ¢å¤Successfully
        logger.info(f"æ¢å¤åé…ç½®: å…¨å±€={ParallelConfig._config['max_workers']}, " +
                   f"å›¾å½¢æ„å»º={ParallelConfig._config['default_workers']['graph_builder']}")
        return True
    except Exception as e:
        logger.error(f"é…ç½®æ›´æ–°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


#--------------------------------------------------------------------
# TODO: Translate - æŠ¥å‘ŠGenerateç›¸å…³åŠŸèƒ½
#--------------------------------------------------------------------
def generate_parallel_report():
    """
    ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š
    
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    # InitializeparallelConfigure
    ParallelConfig.initialize()
    
    # TODO: Translate - Initializethreadç›‘æ§
    thread_monitor = ThreadUsageMonitor.get_instance()
    
    # TODO: Translate - CreateæŠ¥å‘Šç›®å½•
    report_dir = Path("reports")
    report_dir.mkdir(exist_ok=True)
    
    report_file = report_dir / f"parallel_config_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    # TODO: Translate - æ”¶é›†Configureä¿¡æ¯
    config_info = {
        "enabled": ParallelConfig.is_enabled(),
        "max_workers": ParallelConfig._config["max_workers"],
        "adaptive": ParallelConfig._config["adaptive"],
        "default_workers": ParallelConfig._config["default_workers"]
    }
    
    # TODO: Translate - GenerateparallelConfigureæŠ¥å‘Š
    logging.info("ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š...")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# TODO: Translate - ç³»ç»ŸparallelProcessConfigureæŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # TODO: Translate - åŸºæœ¬Configure
        f.write("# TODO: Translate - åŸºæœ¬Configure\n\n")
        f.write(f"- å¹¶è¡Œå¤„ç†çŠ¶æ€: {'å¯ç”¨' if config_info['enabled'] else 'ç¦ç”¨'}\n")
        f.write(f"- å…¨å±€æœ€å¤§çº¿ç¨‹æ•°: {config_info['max_workers']}\n")
        
        # TODO: Translate - è‡ªé€‚åº”Configure
        f.write("\n# TODO: Translate - è‡ªé€‚åº”threadConfigure\n\n")
        adaptive = config_info['adaptive']
        f.write(f"- è‡ªé€‚åº”æ¨¡å¼: {'å¯ç”¨' if adaptive['enabled'] else 'ç¦ç”¨'}\n")
        if adaptive['enabled']:
            f.write(f"- IOå¯†é›†å‹ä»»åŠ¡ç³»æ•°: {adaptive['io_bound_factor']}\n")
            f.write(f"- CPUå¯†é›†å‹ä»»åŠ¡ç³»æ•°: {adaptive['cpu_bound_factor']}\n")
            
            io_threads = int(config_info['max_workers'] * adaptive['io_bound_factor'])
            cpu_threads = int(config_info['max_workers'] * adaptive['cpu_bound_factor'])
            
            f.write(f"- IOå¯†é›†å‹ä»»åŠ¡çº¿ç¨‹æ•°: {io_threads}\n")
            f.write(f"- CPUå¯†é›†å‹ä»»åŠ¡çº¿ç¨‹æ•°: {cpu_threads}\n")
        
        # TODO: Translate - æ¨¡å—ç‰¹å®šConfigure
        f.write("\n# TODO: Translate - æ¨¡å—ç‰¹å®šConfigure\n\n")
        f.write("| æ¨¡å— | é…ç½®çº¿ç¨‹æ•° |\n")
        f.write("|------|----------|\n")
        
        for module, workers in config_info['default_workers'].items():
            f.write(f"| {module} | {workers} |\n")
        
        # TODO: Translate - Initializeå„æ¨¡å—å¹¶è®°å½•threadUse
        f.write("\n# TODO: Translate - å®é™…threadUseæƒ…å†µ\n\n")
        f.write("ç°åœ¨å¼€å§‹æµ‹è¯•å„æ¨¡å—å®é™…ä½¿ç”¨çš„çº¿ç¨‹æ•°...\n\n")
        
        # eventExtract
        logging.info("æµ‹è¯•äº‹ä»¶æŠ½å–æ¨¡å—...")
        f.write("# TODO: Translate - eventExtractæ¨¡å—\n\n")
        try:
            from event_extraction.di.provider import provide_extractor
            extractor = provide_extractor()
            f.write("âœ… äº‹ä»¶æŠ½å–æ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ äº‹ä»¶æŠ½å–æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # hallucinationrefine
        logging.info("æµ‹è¯•å¹»è§‰ä¿®å¤æ¨¡å—...")
        f.write("\n# TODO: Translate - hallucinationrefineæ¨¡å—\n\n")
        try:
            from hallucination_refine.di.provider import provide_refiner
            refiner = provide_refiner()
            f.write("âœ… å¹»è§‰ä¿®å¤æ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ å¹»è§‰ä¿®å¤æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # causallinking
        logging.info("æµ‹è¯•å› æœé“¾æ¥æ¨¡å—...")
        f.write("\n# TODO: Translate - causallinkingæ¨¡å—\n\n")
        try:
            from causal_linking.di.provider import provide_linker
            linker = provide_linker()
            f.write("âœ… å› æœé“¾æ¥æ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ å› æœé“¾æ¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # TODO: Translate - å›¾å½¢Build
        logging.info("æµ‹è¯•å›¾å½¢æ„å»ºæ¨¡å—...")
        f.write("\n# TODO: Translate - å›¾å½¢Buildæ¨¡å—\n\n")
        try:
            from graph_builder.service.mermaid_renderer import MermaidRenderer
            renderer = MermaidRenderer()
            f.write("âœ… å›¾å½¢æ„å»ºæ¨¡å—åˆå§‹åŒ–æˆåŠŸ\n\n")
        except Exception as e:
            f.write(f"âŒ å›¾å½¢æ„å»ºæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}\n\n")
        
        # TODO: Translate - Getå¹¶è®°å½•threadç›‘æ§ä¿¡æ¯
        usage_info = thread_monitor.thread_usage
        
        f.write("\n# TODO: Translate - threadUseæ‘˜è¦\n\n")
        f.write("| æ¨¡å— | é…ç½®çº¿ç¨‹æ•° | å®é™…ä½¿ç”¨çº¿ç¨‹æ•° | ä»»åŠ¡ç±»å‹ |\n")
        f.write("|------|------------|--------------|--------|\n")
        
        for module, workers in config_info['default_workers'].items():
            actual_workers = usage_info.get(module, {}).get("thread_count", "æœªçŸ¥")
            task_type = usage_info.get(module, {}).get("task_type", "æœªçŸ¥")
            f.write(f"| {module} | {workers} | {actual_workers} | {task_type} |\n")
        
        # TODO: Translate - ç»“è®ºå’Œå»ºè®®
        f.write("\n# TODO: Translate - ç»“è®ºå’Œå»ºè®®\n\n")
        
        # TODO: Translate - Checkæ˜¯å¦æœ‰æ¨¡å—æœªUseé›†ä¸­Configure
        unconfigured_modules = set(usage_info.keys()) - set(config_info['default_workers'].keys())
        if unconfigured_modules:
            f.write("âš ï¸ ä»¥ä¸‹æ¨¡å—æœªä½¿ç”¨ä¸­å¤®é…ç½®:\n\n")
            for module in unconfigured_modules:
                f.write(f"- {module}\n")
            f.write("\nå»ºè®®å°†è¿™äº›æ¨¡å—æ·»åŠ åˆ°ä¸­å¤®é…ç½®ä¸­ã€‚\n\n")
        
        # TODO: Translate - CheckConfigureä¸Useæ˜¯å¦ä¸€è‡´
        inconsistent_modules = []
        for module, info in usage_info.items():
            if module in config_info['default_workers']:
                expected = config_info['default_workers'][module]
                actual = info.get("thread_count", 0)
                if expected != actual and ParallelConfig.is_enabled():
                    inconsistent_modules.append((module, expected, actual))
                    
        if inconsistent_modules:
            f.write("âš ï¸ ä»¥ä¸‹æ¨¡å—çš„çº¿ç¨‹ä½¿ç”¨ä¸é…ç½®ä¸ä¸€è‡´:\n\n")
            for module, expected, actual in inconsistent_modules:
                f.write(f"- {module}: æœŸæœ› {expected}ï¼Œå®é™… {actual}\n")
            f.write("\nå»ºè®®æ£€æŸ¥è¿™äº›æ¨¡å—çš„å¹¶è¡Œå®ç°æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†ParallelConfigã€‚\n\n")
        
        # TODO: Translate - é€‚åº”æ€§å»ºè®®
        f.write("# TODO: Translate - ä¼˜åŒ–å»ºè®®\n\n")
        f.write("æ ¹æ®æ¨¡å—ä»»åŠ¡ç‰¹æ€§çš„ä¸åŒï¼Œå»ºè®®ä»¥ä¸‹çº¿ç¨‹é…ç½®ï¼š\n\n")
        f.write("- IOå¯†é›†å‹ä»»åŠ¡ (å¦‚APIè°ƒç”¨): æ ¸å¿ƒæ•° x 1.5\n")
        f.write("- CPUå¯†é›†å‹ä»»åŠ¡ (å¦‚å›¾å½¢æ¸²æŸ“): æ ¸å¿ƒæ•° x 0.8\n")
        f.write("- æ··åˆå‹ä»»åŠ¡: ä¸æ ¸å¿ƒæ•°ç›¸å½“\n\n")
        
        f.write("å½“å‰ç³»ç»Ÿä¸­çš„åˆ†ç±»ï¼š\n\n")
        f.write("- IOå¯†é›†å‹ï¼ševent_extraction, hallucination_refine\n")
        f.write("- CPUå¯†é›†å‹ï¼šgraph_builder\n")
        f.write("- æ··åˆå‹ï¼šcausal_linking\n")
    
    logging.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"ğŸ“Š å¹¶è¡Œé…ç½®æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    return report_file


#--------------------------------------------------------------------
# TODO: Translate - åŸºå‡†Testç›¸å…³åŠŸèƒ½
#--------------------------------------------------------------------
def test_event_extraction(chapter_file):
    """
    æµ‹è¯•äº‹ä»¶æå–æ¨¡å—
    
    Args:
        chapter_file: ç« èŠ‚æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„äº‹ä»¶
    """
    from text_ingestion.chapter_loader import ChapterLoader
    from event_extraction.di.provider import provide_extractor
    
    # Loadchapter
    loader = ChapterLoader(segment_size=800)
    chapter = loader.load_from_json(chapter_file)
    
    if not chapter:
        raise ValueError("Failed to load chapter")
    
    # Extractevent
    extractor = provide_extractor()
    print(f"ä»ç« èŠ‚ {chapter.chapter_id} æå–äº‹ä»¶...")
    events = extractor.extract(chapter)
    print(f"æˆåŠŸæå– {len(events)} ä¸ªäº‹ä»¶")
    
    return events


def test_hallucination_refine(events, context):
    """
    æµ‹è¯•å¹»è§‰ä¿®å¤æ¨¡å—
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
    Returns:
        Refined event
    """
    from hallucination_refine.di.provider import provide_refiner
    
    refiner = provide_refiner()
    print(f"å¯¹ {len(events)} ä¸ªäº‹ä»¶è¿›è¡Œå¹»è§‰æ£€æµ‹å’Œä¿®å¤...")
    refined_events = refiner.refine(events, context=context)
    print(f"ç²¾ä¿®å®Œæˆï¼Œå…± {len(refined_events)} ä¸ªäº‹ä»¶")
    
    return refined_events


def test_causal_linking(events):
    """
    æµ‹è¯•å› æœåˆ†ææ¨¡å—
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
        
    Returns:
        äº‹ä»¶å’Œè¾¹çš„å…ƒç»„
    """
    from causal_linking.di.provider import provide_linker
    
    linker = provide_linker(use_optimized=True)
    print(f"åˆ†æ {len(events)} ä¸ªäº‹ä»¶ä¹‹é—´çš„å› æœå…³ç³»...")
    edges = linker.link_events(events)
    print(f"å‘ç° {len(edges)} ä¸ªå› æœå…³ç³»")
    
    # BuildDAG
    print("æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰...")
    events, dag_edges = linker.build_dag(events, edges)
    print(f"DAGæ„å»ºå®Œæˆï¼Œä¿ç•™ {len(dag_edges)} æ¡è¾¹")
    
    return events, dag_edges


def test_graph_rendering(events, edges):
    """
    æµ‹è¯•å›¾å½¢æ¸²æŸ“æ¨¡å—
    
    Args:
        events: äº‹ä»¶åˆ—è¡¨
        edges: è¾¹åˆ—è¡¨
        
    Returns:
        æ¸²æŸ“çš„Mermaidæ–‡æœ¬
    """
    from graph_builder.service.mermaid_renderer import MermaidRenderer
    
    renderer = MermaidRenderer()
    options = {
        "show_legend": True,
        "show_edge_labels": True,
        "custom_edge_style": True
    }
    
    print(f"æ¸²æŸ“ {len(events)} ä¸ªäº‹ä»¶èŠ‚ç‚¹å’Œ {len(edges)} æ¡è¾¹...")
    mermaid_text = renderer.render(events, edges, options)
    
    return mermaid_text


def run_benchmark(args):
    """
    è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    # TODO: Translate - Setè¾“å…¥æ–‡ä»¶è·¯å¾„
    if args.input:
        chapter_file = args.input
    else:
        # TODO: Translate - æŸ¥æ‰¾Testæ•°æ®
        temp_dir = os.path.join(project_root, "temp")
        output_dirs = [d for d in os.listdir(os.path.join(project_root, "output")) 
                       if os.path.isdir(os.path.join(project_root, "output", d))]
        
        if output_dirs:
            # TODO: Translate - Useæœ€æ–°çš„Outputç›®å½•
            latest_dir = sorted(output_dirs)[-1]
            temp_dir = os.path.join(project_root, "output", latest_dir, "temp")
            
        # TODO: Translate - æŸ¥æ‰¾chapterJSONæ–‡ä»¶
        json_files = [f for f in os.listdir(temp_dir) 
                     if os.path.isfile(os.path.join(temp_dir, f))
                     and f.endswith('.json') and 'chapter' in f.lower()]
        
        if not json_files:
            # TODO: Translate - å°è¯•æŸ¥æ‰¾ä»»æ„JSONæ–‡ä»¶
            json_files = [f for f in os.listdir(temp_dir) 
                         if os.path.isfile(os.path.join(temp_dir, f))
                         and f.endswith('.json')]
            
        if not json_files:
            raise FileNotFoundError("æ‰¾ä¸åˆ°æµ‹è¯•ç”¨çš„ç« èŠ‚JSONæ–‡ä»¶")
            
        # TODO: Translate - Useç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„JSONæ–‡ä»¶
        chapter_file = os.path.join(temp_dir, json_files[0])
        
    print(f"ä½¿ç”¨æµ‹è¯•æ•°æ®: {chapter_file}")
    
    # TODO: Translate - Runparallelæ¨¡å¼Test
    print("===== å¹¶è¡Œæ¨¡å¼æµ‹è¯• =====")
    
    # TODO: Translate - ç¡®ä¿parallelæ¨¡å¼å·²å¯ç”¨
    ParallelConfig.initialize({"enabled": True})
    print(f"å¹¶è¡Œæ¨¡å¼: å¯ç”¨ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {ParallelConfig._config['max_workers']}")
    
    # TODO: Translate - Saveå„Testé˜¶æ®µçš„Executeæ—¶é—´
    parallel_results = {}
    
    # TODO: Translate - eventExtracté˜¶æ®µ
    events, duration = run_module_test("äº‹ä»¶æŠ½å–", test_event_extraction, chapter_file)
    parallel_results["äº‹ä»¶æŠ½å–"] = duration
    
    if not events:
        print("äº‹ä»¶æŠ½å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
    
    # TODO: Translate - Extractchapterä¸Šä¸‹æ–‡ç”¨äºhallucinationæ£€æµ‹
    context = "æµ‹è¯•ä¸Šä¸‹æ–‡"
    try:
        with open(chapter_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict) and "content" in data:
                context = data["content"][:500] + "..." if len(data["content"]) > 500 else data["content"]
    except:
        print("æå–ä¸Šä¸‹æ–‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡")
    
    # TODO: Translate - hallucinationrefineé˜¶æ®µ
    refined_events, duration = run_module_test("å¹»è§‰ä¿®å¤", test_hallucination_refine, events, context)
    parallel_results["å¹»è§‰ä¿®å¤"] = duration
    
    if not refined_events:
        refined_events = events
    
    # TODO: Translate - causallinkingé˜¶æ®µ
    linking_result, duration = run_module_test("å› æœé“¾æ¥", test_causal_linking, refined_events)
    parallel_results["å› æœé“¾æ¥"] = duration
    
    if not linking_result:
        print("å› æœé“¾æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
        
    events, edges = linking_result
    
    # TODO: Translate - å›¾è°±æ¸²æŸ“é˜¶æ®µ
    mermaid_text, duration = run_module_test("å›¾è°±æ¸²æŸ“", test_graph_rendering, events, edges)
    parallel_results["å›¾è°±æ¸²æŸ“"] = duration
    
    # TODO: Translate - å¦‚æœä¸è·³è¿‡é¡ºåºTestï¼Œåˆ™è¿›è¡Œé¡ºåºæ¨¡å¼Test
    sequential_results = {}
    if not args.skip_sequential:
        print("\n===== é¡ºåºæ¨¡å¼æµ‹è¯• =====")
        
        # TODO: Translate - åˆ‡æ¢åˆ°é¡ºåºæ¨¡å¼
        ParallelConfig.initialize({"enabled": False})
        print("é¡ºåºæ¨¡å¼: å¯ç”¨")
        
        # TODO: Translate - åŒæ ·çš„Testæµç¨‹
        events, duration = run_module_test("äº‹ä»¶æŠ½å–(é¡ºåº)", test_event_extraction, chapter_file)
        sequential_results["äº‹ä»¶æŠ½å–"] = duration
        
        if not events:
            print("äº‹ä»¶æŠ½å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # TODO: Translate - hallucinationrefineé˜¶æ®µ
        refined_events, duration = run_module_test("å¹»è§‰ä¿®å¤(é¡ºåº)", test_hallucination_refine, events, context)
        sequential_results["å¹»è§‰ä¿®å¤"] = duration
        
        if not refined_events:
            refined_events = events
        
        # TODO: Translate - causallinkingé˜¶æ®µ
        linking_result, duration = run_module_test("å› æœé“¾æ¥(é¡ºåº)", test_causal_linking, refined_events)
        sequential_results["å› æœé“¾æ¥"] = duration
        
        if not linking_result:
            print("å› æœé“¾æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
            
        events, edges = linking_result
        
        # TODO: Translate - å›¾è°±æ¸²æŸ“é˜¶æ®µ
        mermaid_text, duration = run_module_test("å›¾è°±æ¸²æŸ“(é¡ºåº)", test_graph_rendering, events, edges)
        sequential_results["å›¾è°±æ¸²æŸ“"] = duration
    
    # TODO: Translate - é‡æ–°å¯ç”¨parallelæ¨¡å¼
    ParallelConfig.initialize({"enabled": True})
    
    # TODO: Translate - GenerateæŠ¥å‘Š
    report_content = generate_benchmark_report(parallel_results, sequential_results)
    
    # TODO: Translate - CreateæŠ¥å‘Šç›®å½•
    report_dir = Path("reports")
    report_dir.mkdir(exist_ok=True)
    
    # TODO: Translate - WriteæŠ¥å‘Š
    report_file = None
    if args.output:
        report_file = args.output
    else:
        report_file = report_dir / f"parallel_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
        
    print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
    return report_file


def generate_benchmark_report(parallel_results, sequential_results):
    """
    ç”Ÿæˆæ€§èƒ½æ¯”è¾ƒæŠ¥å‘Š
    
    Args:
        parallel_results: å¹¶è¡Œæ¨¡å¼æµ‹è¯•ç»“æœ
        sequential_results: é¡ºåºæ¨¡å¼æµ‹è¯•ç»“æœ
        
    Returns:
        æŠ¥å‘Šæ–‡æœ¬å†…å®¹
    """
    report = []
    report.append("# TODO: Translate - parallelProcessæ€§èƒ½åŸºå‡†TestæŠ¥å‘Š")
    report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # TODO: Translate - æ€§èƒ½æ‘˜è¦
    report.append("# TODO: Translate - æ€§èƒ½æ‘˜è¦")
    total_parallel = sum(parallel_results.values())
    total_sequential = sum(sequential_results.values()) if sequential_results else 0
    
    report.append(f"- å¹¶è¡Œå¤„ç†æ€»è€—æ—¶: {format_duration(total_parallel)}")
    if sequential_results:
        speedup = (total_sequential / total_parallel) if total_parallel > 0 else 0
        report.append(f"- é¡ºåºå¤„ç†æ€»è€—æ—¶: {format_duration(total_sequential)}")
        report.append(f"- åŠ é€Ÿæ¯”: {speedup:.2f}x")
        report.append(f"- æ€§èƒ½æå‡: {(speedup - 1) * 100:.2f}%")
    else:
        report.append("- é¡ºåºå¤„ç†æµ‹è¯•å·²è·³è¿‡")
    report.append("")
    
    # TODO: Translate - æ¨¡å—æ€§èƒ½æ¯”è¾ƒ
    if sequential_results:
        report.append("# TODO: Translate - å„æ¨¡å—æ€§èƒ½æ¯”è¾ƒ")
        report.append("| æ¨¡å— | å¹¶è¡Œå¤„ç†è€—æ—¶ | é¡ºåºå¤„ç†è€—æ—¶ | åŠ é€Ÿæ¯” | æå‡ç™¾åˆ†æ¯” |")
        report.append("| --- | ------- | ------- | ----- | ------- |")
        
        for module in parallel_results.keys():
            par_time = parallel_results[module]
            seq_time = sequential_results.get(module, 0)
            if seq_time > 0 and par_time > 0:
                mod_speedup = seq_time / par_time
                improvement = (mod_speedup - 1) * 100
                report.append(f"| {module} | {format_duration(par_time)} | {format_duration(seq_time)} | {mod_speedup:.2f}x | {improvement:.2f}% |")
    else:
        report.append("# TODO: Translate - parallelæ¨¡å¼Executeæ—¶é—´")
        report.append("| æ¨¡å— | å¹¶è¡Œå¤„ç†è€—æ—¶ |")
        report.append("| --- | ------- |")
        for module, time in parallel_results.items():
            report.append(f"| {module} | {format_duration(time)} |")
    
    report.append("")
    report.append("# TestConfigure")
    report.append(f"- CPUæ ¸å¿ƒæ•°: {os.cpu_count()}")
    report.append(f"- å¹¶è¡Œæ¨¡å¼å·¥ä½œçº¿ç¨‹æ•°:")
    report.append(f"  - äº‹ä»¶æå–: {ParallelConfig.get_max_workers('io_bound')}")
    report.append(f"  - å¹»è§‰ä¿®å¤: {ParallelConfig.get_max_workers('io_bound')}")
    report.append(f"  - å› æœåˆ†æ: {ParallelConfig.get_max_workers()}")
    report.append(f"  - å›¾è°±æ¸²æŸ“: {ParallelConfig.get_max_workers('cpu_bound')}")
    
    return '\n'.join(report)


def main():
    """ç¨‹åºä¸»å…¥å£"""
    # TODO: Translate - è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="å¹¶è¡Œå¤„ç†å·¥å…·")
    parser.add_argument("mode", choices=["test", "bench", "report", "all"], default="all", 
                        nargs="?", help="è¿è¡Œæ¨¡å¼ï¼štest-é…ç½®æµ‹è¯•ï¼Œbench-æ€§èƒ½æµ‹è¯•ï¼Œreport-ç”ŸæˆæŠ¥å‘Šï¼Œall-å…¨éƒ¨è¿è¡Œ")
    parser.add_argument("--input", "-i", help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (ç”¨äºæ€§èƒ½æµ‹è¯•)")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--skip-sequential", action="store_true", help="è·³è¿‡é¡ºåºå¤„ç†æµ‹è¯•")
    args = parser.parse_args()
    
    # TODO: Translate - Setæ—¥å¿—
    logger = setup_logging()
    logger.info(f"è¿è¡Œå¹¶è¡Œå¤„ç†å·¥å…·ï¼Œæ¨¡å¼: {args.mode}")
    
    # TODO: Translate - è®°å½•å¯åŠ¨æ—¶é—´
    start_time = time.time()
    
    # TODO: Translate - æ ¹æ®æ¨¡å¼Runä¸åŒåŠŸèƒ½
    try:
        mode = ParallelToolMode(args.mode)
        
        if mode in [ParallelToolMode.TEST, ParallelToolMode.ALL]:
            logger.info("==== è¿è¡Œå¹¶è¡Œé…ç½®æµ‹è¯• ====")
            test_result = test_parallel_config_consistency(logger)
            if test_result:
                config_result = test_config_updates(logger)
                if config_result:
                    print("âœ… é…ç½®æµ‹è¯•æˆåŠŸ")
                else:
                    print("âŒ é…ç½®æ›´æ–°æµ‹è¯•å¤±è´¥")
            else:
                print("âŒ é…ç½®ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥")
                
        if mode in [ParallelToolMode.REPORT, ParallelToolMode.ALL]:
            logger.info("==== ç”Ÿæˆå¹¶è¡Œé…ç½®æŠ¥å‘Š ====")
            report_file = generate_parallel_report()
            print(f"âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {report_file}")
            
        if mode in [ParallelToolMode.BENCHMARK, ParallelToolMode.ALL]:
            logger.info("==== è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯• ====")
            benchmark_file = run_benchmark(args)
            print(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ: {benchmark_file}")
            
    except Exception as e:
        logger.error(f"è¿è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        
    # TODO: Translate - è®°å½•æ€»Executeæ—¶é—´
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {format_duration(duration)}")
    print(f"æ€»æ‰§è¡Œæ—¶é—´: {format_duration(duration)}")


if __name__ == "__main__":
    main()
