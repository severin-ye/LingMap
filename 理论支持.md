

## 摘要

将小说自动改编成剧本（screenplays）对于电视、电影或歌剧（opera）行业以低成本推广产品至关重要。大型语言模型（LLMs）在长文本生成方面的强大表现，促使我们提出一个基于LLM的框架**Reader-Rewriter (R2)** 来完成这项任务。然而，这里存在两个基本挑战。首先，LLM的**幻觉（hallucinations）** 可能导致情节提取（plot extraction）和剧本生成的不一致（inconsistent）。其次，需要有效地提取嵌入因果关系（causality-embedded）的情节线（plot lines）以实现连贯的重写（coherent rewriting）。因此，我们提出了两种相应的策略：1）一种**幻觉感知精炼方法（hallucination-aware refinement method，HAR）** ，用于迭代地发现和消除幻觉的影响；2）一种基于贪婪循环打破算法（greedy cycle-breaking algorithm）的**因果情节图构建方法（causal plot-graph construction method，CPC）** ，以有效构建具有事件因果关系的情节线。

R2框架利用这些高效的技术，通过两个模块来模仿人类剧本重写过程：**阅读器模块（Reader module）** 采用滑动窗口（sliding window）和CPC来构建因果情节图（causal plot graphs），而**重写器模块（Rewriter module）** 则首先根据这些图生成场景大纲（scene outlines），然后生成剧本（screenplays）。HAR被集成到这两个模块中，以实现LLMs的准确推理（accurate inferences）。实验结果表明了R2的优越性，在GPT-4o的整体胜率（overall win rate）配对比较中，R2显著优于现有三种方法（绝对增长分别为51.3%、22.6%和57.1%）。

---

## 1 引言

剧本是电视剧、电影或歌剧等形式的基础，它们通常直接改编自小说。例如，2007年至2016年间英国制作的前20部电影中，有52%是根据小说改编的（Association & Economics, 2018），而2024年前9个月美国电视或电影改编作品的月平均数量超过10部（Vulture, 2024）。通常，将小说改编成剧本需要专业作家长期付出努力。自动执行这项任务可以显著降低制作成本，并促进这些作品的传播（Zhu et al., 2023）。然而，当前的工作（Zhu et al., 2022; Mirowski et al., 2023; Han et al., 2024; Morris et al., 2023）只能从预定义的大纲生成剧本。因此，这种自动小说到剧本生成（**novel-to-screenplay generation, N2SG**）任务备受期待。

考虑到大型语言模型（LLMs）在文本生成和理解任务中的卓越表现（Brown et al., 2020; Ouyang et al., 2022），我们对基于大型语言模型（LLM）的方法来执行N2SG任务很感兴趣。然而，在构建这样一个系统之前，存在两个基本挑战。

1) **如何在N2SG中消除幻觉（hallucinations）的影响？** 目前的LLMs，如GPT-4，在处理整部小说时面临困难，并且由于LLM的幻觉（Liu et al., 2024; Ji et al., 2023; Shi et al., 2023），在处理冗长输入时经常生成各种不一致（inconsistent）的内容。现有的精炼方法只能粗略地减少这种不一致性，无法处理长输入数据（Madaan et al., 2023; Peng et al., 2023）。

2) **如何提取捕获事件之间复杂因果关系（causal relationships）的有效情节线（plot lines）？** 单纯消除不一致性并不能确保生成的故事具有连贯性（coherence）和与原小说一致的准确情节线。现有的基于情节图（plot graph）的方法（Weyhrauch, 1997; Li et al., 2013）以线性有序事件（linearly ordered events）来描绘情节线。然而，事件可能错综复杂，这些方法无法建模复杂的因果关系。

针对第一个挑战，我们可以只对幻觉引起的不一致性（inconsistency）相关的上下文（context）进行部分精炼。因此，本研究提出了一种**幻觉感知精炼方法（hallucination-aware refinement method，HAR）** ，用于迭代地消除LLM幻觉的影响，以便从长文本中更好地进行信息提取和生成。

针对第二个挑战，需要提取包含因果关系（causalities）的情节线以进行连贯的重写。情节图（plot graphs）便于表示顺序事件，并且可以扩展为**因果情节图（causal plot graphs）** 以嵌入因果关系。因此，本文提出了一种**因果情节图构建（causal plot-graph construction，CPC）** 方法，以稳健地（robustly）提取事件的因果关系，并构建因果情节图。

现在的问题是如何使用HAR和CPC构建一个N2SG系统。观察人类编剧（图1），我们发现他们可以通过阅读和重写组合的过程成功完成这项任务（McKee, 1999）：首先，他们阅读小说以提取关键情节事件（key plot events）和人物档案（character profiles）（即人物传记及其关系），以构建小说的情节线（plot lines）；然后，他们根据这些情节线将小说重写成剧本，这些情节线被改编为故事线（story lines）和场景目标（scene goals），作为指导剧本创作的大纲。阅读和重写步骤可能会多次进行，并进行多次精炼，直到满意为止。

---

**图1：人类编剧的典型重写过程。** 编剧在将小说改编成剧本时，需要多次阅读和重写，并进行迭代精炼。

---

受基于迭代精炼（iterative-refinement）的人类重写过程的启发，我们提出了**Reader-Rewriter (R2) 框架**（图2）。**阅读器（Reader）** 采用基于滑动窗口（sliding window）的策略来扫描整部小说，跨越章节边界，以便有效地捕获事件和人物档案，用于后续的CPC过程来构建因果情节图，其中HAR用于提取准确的事件和人物档案。**重写器（Rewriter）** 采用两步策略，首先获取所有场景的故事线和目标作为全局指导，然后逐场景生成剧本，并在HAR的精确精炼下，确保场景之间的连贯性和一致性。

在由几对小说-剧本组成的数据集上对R2进行了实验，并基于所提出的七个方面进行评估。基于GPT-4o的评估表明，R2在所有方面都显著优于现有方法，并且相对于三种对比方法，整体绝对提升了51.3%、22.6%和57.1%。人类评估者也同样证实了R2的强大性能，证明了其在N2SG任务中的优越性。

总之，本工作的主要贡献如下：

1) **幻觉感知精炼方法（Hallucination-aware refinement method，HAR）** ：用于精炼LLM输出，可以消除LLM幻觉引起的不一致性，并提高LLMs的适用性。

2) **因果情节图构建方法（A causal plot-graph construction method，CPC）** ：采用贪婪循环打破算法（greedy cycle-breaking algorithm），用于提取不包含循环和低强度事件关系的嵌入因果关系的情节图。

3) **基于LLM的N2SG框架R2**：该框架采用HAR和CPC，并通过阅读器（Reader）和重写器（Rewriter）模块模仿人类剧本重写过程，实现自动化的基于因果情节图的剧本生成。

---

**图2：Reader-Rewriter (R2) 的结构。** R2的总体流程 (a) 由两个模块组成：阅读器（Reader）和重写器（Rewriter）。其中，集成了两种策略：幻觉感知精炼（Hallucination-Aware Refinement, HAR）(b) 和因果情节图构建（Causal Plot-graph Construction, CPC）(c)，以高效利用LLMs并理解情节线。箭头表示不同模块之间的数据流。图中示例仅用于更好说明。

---

## 2 基于LLM的小说到剧本生成的基础

基于LLM的N2SG存在两个挑战。首先，由于幻觉（hallucinations），LLM的输出可能与预期结果大相径庭。因此，LLMs可能会提取并生成不存在的事件和剧本。其次，理解小说情节线对于生成连贯（coherent）和一致（consistent）的剧本至关重要。情节图（Plot graphs）常用于描述情节线，它们应该捕获事件之间复杂的因果关系（causalities）。为了解决第一个挑战，引入了**幻觉感知精炼方法（hallucination-aware refinement method, HAR）** ，从而可以显著减轻LLM幻觉的影响（2.1节）。对于第二个挑战，提出了一种**因果情节图构建方法（causal plot-graph construction method）** ，以有效地构建嵌入因果关系（causalities）的情节图（2.2节）。

### 2.1 幻觉感知精炼

HAR促使LLM识别由幻觉引起的内在不一致性（intrinsic inconsistencies），定位幻觉在LLM输出中的发生位置（locate where the hallucinations occur），并提供精炼建议（suggestions for refinement）。

我们用 $\mathcal{M}$ 表示大型语言模型（LLM）。在第 $t$ 轮中，HAR（图2 (b)）首先识别输入 $x_t$ 中出现内在不一致性的幻觉位置 $loc_t$，并生成建议 $sug_t$，描述 $\mathcal{M}$ 如何精炼它们。然后，根据幻觉位置从输入和相应的支持文本（support texts）中提取**幻觉感知上下文（hallucination-aware context）** $c_t$，并输入到 $\mathcal{M}$ 中以精炼 $x_t$ 中的幻觉部分 $r_t$。接下来，将 $r_t$ 合并到 $x_t$ 中作为 $x_{t+1}$，用于第 $t+1$ 轮的自精炼（self-refinement）。这个自精炼过程持续进行，直到初始输入数据完全处理完毕并保持一致，最终得到精炼后的输出。算法1展示了HAR的完整过程。

---

**算法1 幻觉感知精炼**

**输入：**
* $x_0$: 初始输入（Initial input）
* $s$: 支持文本（support text）
* $\mathcal{M}$: 大型语言模型（LLM）
* $\{p_{fb}, p_{refine}\}$: 提示（prompts），其中 $p_{fb}$ 是任务特定的反馈提示， $p_{refine}$ 是输入-输出-反馈精炼四元组示例
* $stop(\cdot)$: 停止条件函数（stop condition function）
* $retrieve(\cdot)$: 上下文检索函数（context retrieve function）

**输出：**
* $x_t$: 精炼且一致的输出（refined and consistent output）

0:  **初始化：**
    * $x_0$, $s$, $\mathcal{M}$, $\{p_{fb}, p_{refine}\}$, $stop(\cdot)$, $retrieve(\cdot)$

1:  **对于每次迭代 $t \in \{0, 1, \dots\}$ 执行：**
2:     $loc_t, sug_t \leftarrow \mathcal{M}(p_{fb} \parallel x_t)$
    * ▷ **定位幻觉并提供建议**（Locate the hallucinations and provide suggestions）
    * _这个步骤就像一个纠错员，让LLM自己去找出它在 $x_t$ 里哪里说错了，并给出修改意见。_
3:     $c_t \leftarrow retrieve(loc_t, s)$
    * ▷ **检索幻觉感知上下文**（Retrieve the hallucination-aware context）
    * _根据找到的错误位置 $loc_t$，从原始支持文本 $s$ 中提取与错误相关的部分，形成上下文 $c_t$。_
4:     **如果 $stop(loc_t, sug_t, t)$ 为真，则：**
5:        **中断**（break）
    * ▷ **检查停止条件**（Check the stop condition）
    * _如果满足了停止条件（比如错误已经很少了，或者迭代次数达到了上限），就停止精炼。_
6:     **否则：**
7:        $r_t \leftarrow \mathcal{M}(p_{refine} \parallel c_t \parallel sug_t)$
    * ▷ **获取输入的精炼部分**（Get the refined part of input）
    * _LLM利用精炼提示 $p_{refine}$、相关的上下文 $c_t$ 和之前给出的建议 $sug_t$ 来生成一个更正后的部分 $r_t$。_
8:        $x_{t+1} \leftarrow Merge(r_t \text{ into } x_t)$
    * ▷ **用精炼部分更新输入**（Update the input with the refined part）
    * _将修正后的部分 $r_t$ 整合回原始输入 $x_t$，形成新的输入 $x_{t+1}$，用于下一轮的精炼。_
9:     **结束如果**
10: **结束对于**

11: **返回** 精炼且一致的输出 $x_t$

---

### 2.2 因果情节图构建

#### 因果情节图（The Causal Plot Graph）

**因果情节图（causal plot graph）**（图3）通过图（graphs）嵌入事件的因果关系（causalities）。这里的“因果（causal）”意味着图是根据关键事件之间的**关键连接（critical connections）** 构建的，而不仅仅是它们在小说中的序列。特别是，这种类型的情节图被设计为**有向无环图（directed acyclic graph，DAG）**，用于表示情节事件（plot events）（节点）之间的因果关系（边缘）。

形式上，因果情节图是一个元组 $G = \langle E, D, W \rangle$。
* $E$ 表示**事件（events）**，由地点和时间、背景和描述组成。
* $D$ 描述事件的**因果关系（causal relationships）**。
* $W$ 表示因果关系的**强度（strengths）**，分为三个级别：
    * **高（High）**：表示直接且显著的影响。
    * **中（Medium）**：表示部分或间接的影响。
    * **低（Low）**：表示最小或微弱的影响。

---

**图3：因果情节图和人物档案的演示。** 情节线（plot lines）表示为由事件（events）及其因果关系（causal relationships）组成的**有向无环图（DAGs）**。箭头越粗表示关系越强。

---

#### 贪婪循环打破算法（The Greedy Cycle-breaking Algorithm）

LLMs最初提取的情节图通常由于LLM的幻觉（hallucinations）而包含**循环（cycles）** 和**低强度关系（low-strength relationships）**。因此，我们提出了一种Prim算法（Prim, 1957）的变体来移除这些循环和不重要的关系。该算法被称为**贪婪循环打破算法（greedy cycle-breaking algorithm）**，它根据关系强度（relationship strength）和事件节点（event node）的度（degree）来打破循环。

具体来说，因果关系首先根据它们的权重 $W$ 从高到低排序，形成 $D$。如果两个关系具有相同的强度级别，则会根据其连接事件端点（event endpoints）度数之和的较低者进行排序，这样可以优先选择更重要节点之间更重要的边缘。我们用 $d(a, b)$ 表示每个方向性关系，其中 $a$ 和 $b$ 分别是 $d$ 的起点和终点。端点 $x$ 的**前向可达集（forward reachable set）** 为 $S_x$。如果 $d \in D$ 的终点 $b$ 已经可以通过先前选择的因果关系边缘到达其起点 $a$，则跳过 $d$ 以避免形成循环。否则，将其添加到边缘集 $F$ 中，并更新 $F$ 中所有边缘的端点 $x$ 的 $S_x$，以反映新的连接，确保集合 $F$ 保持无环。结果，集合 $F$ 构成了有向无环图（DAG）的边缘集，该图保留了最重要的因果关系，同时防止了循环。算法2给出了该算法的详细信息。

---

**算法2 用于因果情节图构建的贪婪循环打破算法**

**输入：**
* $E$: 情节事件集合（set of plot events）
* $D$: 因果关系集合（set of causal relations）
* $W$: 关系强度集合（set of relation strengths）

**输出：**
* $F$: 作为DAG的因果关系边缘集合（causal relation edge set of the DAG）

0: **初始化：** $E, D, W$

1: **对 $D$ 进行排序：**
    * 按照 $W$ 从高到低排序（关系强度优先）
    * 对于相同 $W$ 的关系，按照其连接的事件端点度数之和从少到多排序（连接更多重要节点的边优先）
    * _这个步骤就像给所有因果关系一个优先级列表。越重要的关系（强度高，连接的事件更重要）排在越前面。_

2: **对于 $D$ 中的每条边 $d$ 及其端点 $a, b$（其中 $a, b \in E$）执行：**
    * _这里的 $d(a,b)$ 表示从事件 $a$ 到事件 $b$ 的因果关系。_
3:    **如果 $a \in S_b$ 为真，则：**
    * _这里的 $S_b$ 是指从事件 $b$ 出发，在当前已选择的边集 $F$ 中可以到达的所有事件的集合。如果 $a$ 已经在这个集合里，意味着 $a$ 可以从 $b$ 再次被访问到，这就形成了循环。_
4:       **继续（continue）**
    * ▷ **如果 $a$ 可以从 $b$ 到达，则跳过**（Skip if $a$ is reachable from $b$）
    * _为了避免循环，如果发现添加这条边会形成循环（即 $a$ 已经能从 $b$ 访问到），就跳过这条边。_
5:    **结束如果**
6:    **将 $d$ 添加到 $F$ 中**
    * ▷ **添加到无环边缘集**（Add to acyclic edge set）
    * _如果不会形成循环，就将这条边 $d$ 加入到最终的无环边缘集 $F$ 中。_
7:    **更新 $F$ 中所有边缘的每个端点 $x$ 的 $S_x$**
    * ▷ **更新前向可达集**（Update the forward reachable sets）
    * _添加了新的边后，需要更新每个事件节点可以到达的集合，以便在后续判断是否形成循环时使用。_
8: **结束对于**

9: **返回 $F$ 作为DAG的因果关系边缘集**

---

现在我们讨论基于所提出的两种基本技术构建的R2框架。

## 3 提出的Reader-Rewriter框架

R2（图2 (a)）根据人类的重写过程，由两个主要组件组成：**阅读器（Reader）** 和**重写器（Rewriter）**。前者提取情节事件和人物档案，并构建因果情节图，而后者利用这些图和档案将小说改编成剧本。

### 3.1 基于LLM的阅读器

基于LLM的阅读器包含两个子模块：**人物事件提取（character event extraction）** 和**情节图提取（plot graph extraction）**。

#### 人物事件提取

阅读器首先从小说中识别情节事件，并以逐章（chapter-by-chapter）的方式提取它们，因为LLMs的输入上下文窗口（input context window）有限。在这里，LLMs提取事件元素，例如描述（description）、地点（place）和时间（time）（图3）。这通过提示LLMs生成结构化输出（structured outputs）来实现（Bi et al., 2024）。

为了更好地处理小说的长文本，在事件提取过程中首先引入了一种**基于滑动窗口（sliding window based）** 的技术。通过以章节大小的窗口滑动浏览整部小说，这种策略确保了提取的事件在章节之间保持一致。它也应用于提取每章的人物档案（图2）。然后，采用HAR（2.1节）来减少LLM幻觉（hallucinations）导致的情节事件和人物档案中的不一致性。在这里，LLM被递归地提示识别不一致性，并根据相关的章节上下文进行精炼，从而显著减少事件和档案之间的不一致性。

#### 情节图提取

提取的事件用于通过所提出的CPC方法进一步构建因果情节图。具体来说，首先递归地提示LLM根据相关的章节上下文识别新的因果关系。在图被连接并且没有新的关系添加到其中之后，执行CPC以消除图中的循环和低权重边缘，从而使获得的因果图能够更有效和准确地反映小说中的情节线。

### 3.2 基于LLM的重写器

重写器分为两个后续步骤：第一步是创建所有场景的剧本大纲（screenplay outlines），第二步是迭代生成每个场景的剧本。这两个步骤被打包为两个相应的子模块：**大纲生成（outline generation）** 和**剧本生成（screenplay generation）**。最终的剧本通过HAR进行迭代精炼。

#### 大纲生成

剧本改编大纲可以与情节图（plot graph）和人物档案（character profiles）一起构建（图2），它包括故事核心元素（story core elements）、剧本结构（screenplay structure）和写作计划（writing plan），其中包含每个场景的故事线（storyline）和目标（goal）。采用了三种不同的方法来遍历情节图：**深度优先遍历（depth-first traversal，DFT）**、**广度优先遍历（breadth-first traversal，BFT）** 和**原始章节顺序（original chapter order，Chapter）**，对应三种不同的剧本改编模式，即：
* 基于主要故事情节改编剧本（深度优先）。
* 基于事件的时间顺序改编剧本（广度优先）。
* 基于小说的原始叙事顺序改编剧本。

在生成大纲时，特别是生成场景写作计划时，事件和人物的错位（misalignment）经常发生。因此，R2执行HAR（2.1节）以获取初始剧本改编大纲。这个过程侧重于关键事件和主要人物的对齐，并返回最终的改编大纲。

#### 剧本生成

现在可以根据每个场景的写作计划（图2）编写每个场景，该计划包括故事线、目标、地点和时间以及人物经历。LLM被提示生成每个场景，并提供与场景相关的上下文，其中包括相关章节和先前生成的场景。然后HAR验证生成的场景是否符合写作计划中概述的故事线目标。这种方法确保了生成剧本场景之间的一致性，并保持与小说相关情节线的对齐。

## 4 实验

### 4.1 实验设置

#### 数据集和评估

为了评估N2SG的性能，我们通过手动清理从公共来源收集的小说和剧本对，创建了一个**小说到剧本数据集（novel-to-screenplay dataset）**。小说根据其评分和评论数量分为流行（popular）和非流行（unpopular）两类。为确保公平性，测试集中包含了这两种类型。该数据集将对未来可训练和免训练（train-free）应用的研究开放。

为确保评估平衡，所提出的R2方法采用五部小说作为测试集，其中两部来自流行类别，三部来自非流行类别，且不涉及训练样本。为了进一步最小化一次性阅读长文本引起的主观偏差，我们为每位人类评估者选择了总共15个小说节选，每个节选限制在约1000个token。

在评估中，我们雇佣了15位人类评估者，重点关注七个方面，包括：**趣味性（Interesting）**、**连贯性（Coherent）**、**类人性（Human-like）**、**措辞与语法（Diction and Grammar）**、**转场（Transition）**、**剧本格式合规性（Script Format Compliance）** 和**一致性（Consistency）**。我们设计了通过问卷进行的**配对比较（pairwise comparisons）**。然后将他们的回答汇总以计算每个方面的**胜率（win rate）**（公式1）。然而，由于人类评估者在判断中经常表现出较大的差异，因此我们也利用GPT-4o作为主要评估者，根据相同的问卷给出判断。这可以增强客观性并减少结果中的潜在偏差。附录A提供了数据集和评估的进一步细节。

#### 任务设置

R2框架使用从分析实验（4.4节）中获得的最佳参数，将精炼轮数设置为4，情节图遍历方法设置为BFT，以与竞争对手进行比较。它采用GPT-4o-mini3作为骨干模型，因为它成本低廉且推理速度快，我们的目标是构建一个有效实用的N2SG系统。在推理过程中，生成温度（generation temperature）设置为0，以实现可复现和稳定的生成。

#### 对比方法

R2与ROLLING、Dramatron（Mirowski et al., 2023）和Wawa Writer4进行了比较。
* **ROLLING** 是一种普通的SG（Screenplay Generation）方法，它通过GPT-4o-mini一次生成4096个token，使用R2提取的情节事件和所有先前生成的剧本文本作为提示。一旦生成达到4096个token，它将被添加到提示中以迭代生成剧本。
* **Dramatron** 是一种从提纲（loglines）生成剧本的方法。这里我们输入R2提取的情节事件进行比较。
* **Wawa Writer** 是一种商业AI写作工具，我们采用了其小说到剧本的功能进行性能比较。

---

**表1：R2与三种方法在GPT-4o评估的胜率（%）方面的比较。**

| 方法      | 趣味性（Interesting） | 连贯性（Coherent） | 类人性（Human-like） | 措辞与语法（Dict & Gram） | 转场（Transition） | 格式（Format） | 一致性（Consistency） | 整体（Overall） |
| :-------- | :-------------------- | :----------------- | :------------------- | :---------------------- | :----------------- | :------------- | :-------------------- | :-------------- |
| ROLLING   | 19.2                  | 34.6               | 26.9                 | 15.4                    | 30.8               | 15.4           | 23.1                  | 24.4            |
| R2        | 80.8 (↑61.6)          | 65.4 (↑30.8)       | 73.1 (↑46.2)         | 84.6 (↑69.2)            | 69.2 (↑38.4)       | 84.6 (↑69.2)   | 76.9 (↑53.8)          | 75.6 (↑51.3)    |
| Dramatron | 39.3                  | 46.4               | 35.7                 | 42.9                    | 28.6               | 35.7           | 50.0                  | 39.3            |
| R2        | 60.7 (↑21.4)          | 57.1 (↑10.7)       | 64.3 (↑28.6)         | 57.1 (↑14.2)            | 71.4 (↑42.8)       | 64.3 (↑28.6)   | 57.1 (↑7.1)           | 61.9 (↑22.6)    |
| Wawa Writer | 10.7                  | 32.1               | 25.0                 | 10.7                    | 25.0               | 35.7           | 21.4                  | 22.0            |
| R2        | 89.3 (↑78.6)          | 75.0 (↑42.9)       | 75.0 (↑50.0)         | 89.3 (↑78.6)            | 75.0 (↑50.0)       | 64.3 (↑28.6)   | 78.6 (↑57.1)          | 79.2 (↑57.1)    |

---

**表2：R2与三种方法在人类评估的胜率（%）方面的比较。**

| 方法      | 趣味性（Interesting） | 连贯性（Coherent） | 类人性（Human-like） | 措辞与语法（Dict & Gram） | 转场（Transition） | 格式（Format） | 一致性（Consistency） | 整体（Overall） |
| :-------- | :-------------------- | :----------------- | :------------------- | :---------------------- | :----------------- | :------------- | :-------------------- | :-------------- |
| ROLLING   | 35.9                  | 40.1               | 36.6                 | 19.0                    | 35.2               | 35.2           | 45.1                  | 34.5            |
| R2        | 71.8 (↑35.9)          | 66.9 (↑26.8)       | 73.9 (↑37.3)         | 83.1 (↑64.1)            | 70.4 (↑35.2)       | 88.7 (↑53.5)   | 77.5 (↑32.4)          | 74.9 (↑40.4)    |
| Dramatron | 40.0                  | 47.8               | 48.9                 | 61.1                    | 47.8               | 48.9           | 66.7                  | 50.6            |
| R2        | 74.4 (↑34.4)          | 52.2 (↑4.4)        | 54.4 (↑5.5)          | 40.0 (↓21.1)            | 56.7 (↑8.9)        | 77.8 (↑28.9)   | 55.6 (↓11.1)          | 57.4 (↑6.9)     |
| Wawa Writer | 43.8                  | 40.0               | 47.5                 | 45.0                    | 43.8               | 47.5           | 45.0                  | 44.4            |
| R2        | 62.5 (↑18.7)          | 67.5 (↑27.5)       | 62.5 (↑15.0)         | 62.5 (↑17.5)            | 62.5 (↑18.7)       | 60.0 (↑12.5)   | 50.0 (↑5.0)           | 62.1 (↑17.7)    |

---

### 4.2 主要结果

表1中的定量比较显示，R2始终优于竞争对手（整体而言，R2比ROLLING提高了51.3%，比Dramatron提高了22.6%，比Wawa Writer提高了57.1%）。特别是，R2在**措辞与语法（Dict & Gram）** 方面（比ROLLING高出69.2%）和**趣味性（Interesting）** 方面（比Wawa Writer高出78.6%）表现出明显的优越性。这些结果表明R2可以生成语言准确且精彩的剧本，并具有流畅的转场。此外，表2中的人类评估结果表明，R2在大多数方面整体优于其竞争对手，尤其是在**趣味性（Interesting）** 和**转场（Transition）** 方面，表明其能够生成精彩流畅的剧本。仅与Dramatron相比，R2在**措辞与语法（Dict & Gram）** 和**一致性（Consistency）** 方面表现略差。一个可能的原因是人类偏爱Dramatron生成的长篇叙事。

对生成的剧本进行的**定性分析（qualitative analysis）** 表明，对比方法存在以下缺点：由于缺乏迭代精炼（iterative refinement）以及对小说情节的理解有限，ROLLING生成的剧本在**趣味性（Interesting）**、**转场（Transition）** 和**一致性（Consistency）** 方面通常不如R2。Dramatron倾向于生成类似戏剧的剧本，经常生成冗长的对话，这导致其在**趣味性（Interesting）**、**格式（Format）** 和**转场（Transition）** 方面表现不佳。至于Wawa Writer，它生成的剧本场景之间经常出现情节不一致（plot inconsistencies）和**措辞与语法（Diction and Grammar）** 问题，这表明其骨干模型可能缺乏对小说的深入理解。

### 4.3 消融研究

本研究通过GPT-4o评估了相关技术的有效性（表3）。首先，移除HAR导致**措辞与语法（Dict & Gram）** 显著下降（损失38.4%）和**一致性（Consistency）** 显著下降（损失46.1%），这表明HAR对于提高语言质量和一致性至关重要。其次，移除CPC导致**趣味性（Interesting）** 显著下降（损失64.2%）和**一致性（Consistency）** 显著下降（损失71.4%），这表明CPC在生成精彩和一致的剧本方面至关重要。最后，排除所有上下文支持导致**转场（Transition）** 急剧下降（损失66.6%）和**一致性（Consistency）** 显著下降（损失77.8%），这表明它对于改善情节转场和一致性非常重要。

---

**表3：R2在GPT-4o评估的胜率消融结果。**

| 方法       | 趣味性（Interesting） | 连贯性（Coherent） | 类人性（Human-like） | 措辞与语法（Dict & Gram） | 转场（Transition） | 格式（Format） | 一致性（Consistency） | 整体（Overall） |
| :--------- | :-------------------- | :----------------- | :------------------- | :---------------------- | :----------------- | :------------- | :-------------------- | :-------------- |
| R2         | 61.5                  | 61.5               | 65.4                 | 69.2                    | 61.5               | 61.5           | 76.9                  | 77.7            |
| w/o HAR    | 38.5 (↓23.0)          | 38.5 (↓23.0)       | 38.5 (↓26.9)         | 30.8 (↓38.4)            | 38.5 (↓23.0)       | 46.2 (↓15.3)   | 30.8 (↓46.1)          | 44.7 (↓33.0)    |
| R2         | 82.1                  | 64.3               | 78.6                 | 71.4                    | 82.1               | 78.6           | 85.7                  | 92.1            |
| w/o CPC    | 17.9 (↓64.2)          | 85.7 (↑21.4)       | 21.4 (↓57.2)         | 28.6 (↓42.8)            | 28.6 (↓53.5)       | 64.3 (↓14.3)   | 14.3 (↓71.4)          | 44.3 (↓47.8)    |
| R2         | 66.7                  | 77.8               | 77.8                 | 66.7                    | 83.3               | 88.9           | 88.9                  | 92.2            |
| w/o Context | 33.3 (↓33.4)          | 50.0 (↓27.8)       | 22.2 (↓55.6)         | 33.3 (↓33.4)            | 16.7 (↓66.6)       | 11.1 (↓77.8)   | 11.1 (↓77.8)          | 33.3 (↓58.9)    |

---

### 4.4 不同因素的影响

#### 情节图遍历方法（The Plot Graph Traversal Methods）

图4 (a) 探讨了不同情节图遍历方法对剧本改编的影响。这里直接展示了与Chapter方法相比的胜率差异，因为Chapter方法的表现落后于其他方法。总的来说，**广度优先遍历（BFT）** 优于**深度优先遍历（DFT）**，并在**连贯性（Coherent）**、**转场（Transition）**、**格式（Format）** 和**一致性（Consistency）** 方面表现出显著优势。这说明BFT在讲述情节错综复杂的故事方面非常有效，而DFT则在创作精彩故事方面保持了强大性能。这些结果证实BFT在剧本改编中提供了情节连贯性和整体质量的最佳平衡。

---

**图4：遍历方法和精炼轮数的影响。**
(a) 不同遍历方法分析
(b) 不同精炼轮数分析

---

#### 精炼轮数（The Rounds of Refinements）

图4 (b) 表明，当精炼轮数增加时，建议数量在前四轮中增加，随后开始下降，这表明改进空间变小。建议采纳率呈现下降趋势，在第2到第4轮之间稳定在60%左右，在第5轮中出现显著下降。此外，随着精炼轮数的增加，时间成本也逐步提高。因此，四轮精炼在精炼质量和效率之间取得了最佳平衡。

### 4.5 案例研究

本研究还进行了一个案例研究，以展示R2的有效性，其中图5展示了R2和三种方法生成的剧本片段。R2在创造生动的场景（vivid settings）、富有表现力的对话（expressive dialogue）以及将音乐与人物发展（character development）相结合方面优于其他方法。例如，R2通过优雅的场景设置有效增强了氛围，并通过情感对话强调了沃尔夫冈（Wolfgang）的激情和抱负。这些元素使剧本比其他脚本中更简单的处理方式更具沉浸感和情感驱动力。

---

**图5：不同方法的案例研究。**

---

## 5 相关工作

### 长文本生成（Long-form Generation）

最近，许多研究（Yang & Klein, 2021; Yang et al., 2022; Lei et al., 2024）涌现，关注LLM的长文本生成，旨在解决长文本生成中的长距离依赖（long-range dependency）问题、内容连贯性（content coherence）、前提相关性（premise relevance）和事实一致性（factual consistency）等挑战。Re3（Yang et al., 2022）引入了一个四阶段过程（plan, draft, rewrite, and edit）用于长故事生成，使用递归重提示（recursive reprompting）和修订；DOC（Yang et al., 2023）专注于生成与人物关联的详细大纲的故事，并使用控制器（controller）确保连贯性和控制。与这些由故事大纲驱动的多阶段生成框架相比，我们的方法独特地利用凝练的因果情节图（condensed causal plot graph）和人物档案（character profiles）实现从小说到剧本的自动化和一致生成。

其他工作则侧重于构建人机协作框架（human-AI collaboration frameworks）用于剧本生成（Zhu et al., 2022; Mirowski et al., 2023; Han et al., 2024; Zhu et al., 2023）。Dramatron（Mirowski et al., 2023）提出了一种分层故事生成框架（hierarchical story generation framework），该框架使用提示链（prompt chaining）来指导LLMs生成关键剧本元素，构建了一个用于长篇剧本生成的人机协作系统。IBSEN（Han et al., 2024）允许用户与导演和角色代理（character agents）交互以控制剧本生成过程。这些研究强调协作式、多代理（multi-agent）方法与人-LLM交互。相比之下，我们的方法通过从小说中自动生成长篇剧本，最小化用户参与来解决N2SG。

### 基于LLM的自精炼方法（LLM-Based Self-Refine Approach）

迭代自精炼（Iterative self-refinement）是人类问题解决的基本特征（Simon, 1962）。LLMs也可以通过自精炼来提高其生成质量（Madaan et al., 2023; Saunders et al., 2022; Scheurer et al., 2024; Shinn et al., 2023; Peng et al., 2023; Madaan et al., 2023）。LLM-Augmenter（Peng et al., 2023）使用即插即用模块（plug-and-play module）通过整合外部知识和自动反馈来增强LLM输出。Self-Refine（Madaan et al., 2023）表明LLMs可以通过多轮提示（multi-turn prompting）来改进其在各种生成任务中的输出。在本文中，R2利用基于LLMs的自精炼方法来解决因果情节图提取和长文本生成中的挑战。

## 6 结论

本文提出了一种基于大型语言模型（LLM）的框架R2，用于**小说到剧本生成任务（N2SG）**。首先提出了两种技术：**幻觉感知精炼（HAR）**，用于通过消除幻觉的影响来更好地探索LLMs的能力；以及**因果情节图构建（CPC）**，用于更好地捕获因果事件关系。R2采用这些技术并模仿人类的重写过程，构建了由**阅读器（Reader）** 和**重写器（Rewriter）** 组成的系统，用于情节图提取和逐场景剧本生成。大量的实验表明，R2显著优于竞争对手。R2的成功为N2SG任务建立了基准，并展示了LLMs在将长篇小说改编成连贯剧本方面的潜力。未来的工作可以探索将控制模块（control modules）或多智能体框架（multi-agent frameworks）集成到R2中，以施加更严格的约束，并将其扩展到更广泛的长篇故事生成任务，以进一步发展我们框架的能力。

### 伦理声明（Ethics Statement）

我们的研究使用公开可用的数据，不涉及人类受试者或敏感数据。我们确保我们的工作不会引起偏见、不公平或隐私问题等伦理问题。本研究不存在利益冲突或法律问题，所有程序均符合伦理标准。

### 可复现性声明（Reproducibility Statement）

我们通过提供模型、数据集和实验的全面描述，确保我们结果的可复现性。源代码和数据处理步骤作为补充材料提供。

